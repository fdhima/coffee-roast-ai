{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36d942e8-eb14-4bc8-ac05-417ed0503a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-02 15:31:38.693118: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "25260213-d4d5-4a72-a4d9-bf058114d6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('coffee_beans.csv')\n",
    "X = df[\"filepaths\"]\n",
    "y = df[\"class index\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fe517ecc-4761-43a5-bf0b-6dee0427da03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(x):\n",
    "    images = []\n",
    "    for path in x:\n",
    "        img = cv2.imread(path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (224, 224))\n",
    "        img = img / 255.0  # normalize\n",
    "    \n",
    "        images.append(img)\n",
    "    \n",
    "    images = np.array(images, dtype=np.float32)\n",
    "    return images\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ef8d24-3483-40c5-bf9b-a4ec9b2cc68c",
   "metadata": {},
   "source": [
    "Split dataset into training, validation and test set. This split will be used to choose the best neural network architecture for the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "416ff638-5ed4-4f36-96f4-87cb692b2b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, x_, y_train, y_ = train_test_split(X , y, test_size=0.40, random_state=1)\n",
    "X_cv, X_test, y_cv, y_test = train_test_split(x_ , y_, test_size=0.20, random_state=1)\n",
    "\n",
    "del x_, y_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ce0c5c-4a10-48b7-8a92-54b6dadd6441",
   "metadata": {},
   "source": [
    "Load the images defined in the $x$ sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7c4c62ef-2edb-4c26-b42e-72efcda27fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = load_images(X_train)\n",
    "X_cv = load_images(X_cv)\n",
    "X_test = load_images(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "66e4abe6-394c-4a83-80d9-d19cfab012ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train = df[df[\"filepaths\"].str.startswith(\"train/\")]\n",
    "# df_test = df[df[\"filepaths\"].str.startswith(\"test/\")]\n",
    "\n",
    "# X_train, y_train = load_images(df_train)\n",
    "# X_test, y_test = load_images(df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "733e7b24-4efe-44fb-bfdb-2b98303cbacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.set_random_seed(42) # seed for reproducibility\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "\n",
    "model_1 = Sequential([\n",
    "    Flatten(input_shape=(224, 224, 3)), # ??? have to research\n",
    "    Dense(units=25, activation='relu'),\n",
    "    Dense(units=15, activation='relu'),\n",
    "    Dense(4, activation='linear'), # since the from_logits is used in the loss function\n",
    "], name=\"CoffeeRoastAI_1\")\n",
    "\n",
    "model_2 = Sequential([\n",
    "    Flatten(input_shape=(224, 224, 3)), # ??? have to research\n",
    "    Dense(units=15, activation='relu'),\n",
    "    Dense(units=15, activation='relu'),\n",
    "    Dense(4, activation='linear'), # since the from_logits is used in the loss function\n",
    "], name=\"CoffeeRoastAI_2\")\n",
    "\n",
    "model_3 = Sequential([\n",
    "    Flatten(input_shape=(224, 224, 3)), # ??? have to research\n",
    "    Dense(units=5, activation='relu'),\n",
    "    Dense(units=15, activation='relu'),\n",
    "    Dense(4, activation='linear'), # since the from_logits is used in the loss function\n",
    "], name=\"CoffeeRoastAI_3\")\n",
    "\n",
    "nn_models = [model_1, model_2, model_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c0252840-c8bc-4e11-a059-dcb7466c3443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CoffeeRoastAI_1...\n",
      "Epoch 1/10\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - accuracy: 0.2656 - loss: 1.7781\n",
      "Epoch 2/10\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - accuracy: 0.2385 - loss: 1.3941\n",
      "Epoch 3/10\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.2385 - loss: 1.3916\n",
      "Epoch 4/10\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.2385 - loss: 1.3899\n",
      "Epoch 5/10\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.2385 - loss: 1.3887\n",
      "Epoch 6/10\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.2385 - loss: 1.3878\n",
      "Epoch 7/10\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.2385 - loss: 1.3872\n",
      "Epoch 8/10\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - accuracy: 0.2385 - loss: 1.3867\n",
      "Epoch 9/10\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - accuracy: 0.2438 - loss: 1.3864\n",
      "Epoch 10/10\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - accuracy: 0.2604 - loss: 1.3862\n",
      "Done\n",
      "\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "Training CoffeeRoastAI_2...\n",
      "Epoch 1/10\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - accuracy: 0.2604 - loss: 1.3859\n",
      "Epoch 2/10\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.2604 - loss: 1.3858\n",
      "Epoch 3/10\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.2604 - loss: 1.3858\n",
      "Epoch 4/10\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.2604 - loss: 1.3858\n",
      "Epoch 5/10\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.2604 - loss: 1.3858\n",
      "Epoch 6/10\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.2604 - loss: 1.3858\n",
      "Epoch 7/10\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.2604 - loss: 1.3858\n",
      "Epoch 8/10\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.2604 - loss: 1.3858\n",
      "Epoch 9/10\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.2604 - loss: 1.3858\n",
      "Epoch 10/10\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.2604 - loss: 1.3858\n",
      "Done\n",
      "\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "Training CoffeeRoastAI_3...\n",
      "Epoch 1/10\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.2604 - loss: 1.3859\n",
      "Epoch 2/10\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.2604 - loss: 1.3858\n",
      "Epoch 3/10\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.2604 - loss: 1.3858\n",
      "Epoch 4/10\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.2604 - loss: 1.3858\n",
      "Epoch 5/10\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.2604 - loss: 1.3858\n",
      "Epoch 6/10\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.2604 - loss: 1.3858\n",
      "Epoch 7/10\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.2604 - loss: 1.3858\n",
      "Epoch 8/10\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.2604 - loss: 1.3858\n",
      "Epoch 9/10\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.2604 - loss: 1.3858\n",
      "Epoch 10/10\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.2604 - loss: 1.3858\n",
      "Done\n",
      "\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "nn_train_cross_entropy = []\n",
    "nn_cv_cross_entropy = []\n",
    "\n",
    "for model in nn_models:\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), # gradient descent optimatiation\n",
    "        loss= SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=['accuracy'], # ??? have to research\n",
    "    )\n",
    "    print(f\"Training {model.name}...\")\n",
    "    \n",
    "    model.fit(X_train, y_train, epochs=10)\n",
    "    print(\"Done\\n\")\n",
    "\n",
    "    # Instantiate loss function\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "    # Record the training Log Losses\n",
    "    yhat = model.predict(X_train)\n",
    "    pred_class = np.argmax(yhat, axis=1)[0]\n",
    "    train_cross_entropy = loss_fn(y_train, yhat)\n",
    "    nn_train_cross_entropy.append(train_cross_entropy)\n",
    "\n",
    "    # Record the cross validation Log Losses\n",
    "    yhat = model.predict(X_cv)\n",
    "    pred_class = np.argmax(yhat, axis=1)[0]\n",
    "    cv_cross_entropy = loss_fn(y_cv, yhat)\n",
    "    nn_cv_cross_entropy.append(cv_cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "995c6ed9-c407-4494-9a50-12828ad43cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS:\n",
      "Model 1: Training MSE: 1.60, CV MSE: 1.60\n",
      "Model 2: Training MSE: 4.35, CV MSE: 4.35\n",
      "Model 3: Training MSE: 6.89, CV MSE: 6.89\n"
     ]
    }
   ],
   "source": [
    "print(\"RESULTS:\")\n",
    "for model_num in range(len(nn_train_cross_entropy)):\n",
    "    print(\n",
    "        f\"Model {model_num+1}: Training MSE: {nn_train_cross_entropy[model_num]:.2f}, \" +\n",
    "        f\"CV MSE: {nn_train_cross_entropy[model_num]:.2f}\"\n",
    "        ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed454b9-f3e8-4523-8108-efe7fc605b08",
   "metadata": {},
   "source": [
    "Because softmax is integrated into the output layer the output s a vector of probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4320e64f-b4c7-46cb-ac66-d0d6355f0ef6",
   "metadata": {},
   "source": [
    "Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b4caaf63-1b2f-4d03-b832-2ffc948d7abd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(960, 4), dtype=float32, numpy=\n",
       "array([[0.24802086, 0.23884034, 0.2603363 , 0.25280246],\n",
       "       [0.24802086, 0.23884034, 0.2603363 , 0.25280246],\n",
       "       [0.24802086, 0.23884034, 0.2603363 , 0.25280246],\n",
       "       ...,\n",
       "       [0.24802086, 0.23884034, 0.2603363 , 0.25280246],\n",
       "       [0.24802086, 0.23884034, 0.2603363 , 0.25280246],\n",
       "       [0.24802086, 0.23884034, 0.2603363 , 0.25280246]],\n",
       "      shape=(960, 4), dtype=float32)>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = model(X_train) # outpus z_1, .., z_n and not a_1, .., a_n\n",
    "f_x = tf.nn.softmax(logits) # map the z result to softmax function\n",
    "f_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b4c4742e-b7b2-4400-8895-83ff9fd7d353",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = {\n",
    "    0: \"Dark\",\n",
    "    1: \"Green\",\n",
    "    2: \"Light\",\n",
    "    3: \"Medium\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1f174df5-ffa7-4917-a809-e3ea2af9a919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "Predicted: Light\n",
      "Class probabilities (Dark, Green, Light, Medium): [-0.00444375 -0.04216134  0.04401769  0.01465181]\n"
     ]
    }
   ],
   "source": [
    "IMG_SIZE = 224\n",
    "\n",
    "img = cv2.imread(\"test/Medium/medium (13).png\")\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "img = img / 255.0\n",
    "\n",
    "img = np.expand_dims(img, axis=0)  # shape (1, 224, 224, 3)\n",
    "\n",
    "pred = model.predict(img)\n",
    "pred_class = np.argmax(pred, axis=1)[0]\n",
    "\n",
    "print(\"Predicted:\", class_names[pred_class])\n",
    "print(\"Class probabilities (Dark, Green, Light, Medium):\", pred[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ed5755-261c-405e-8deb-c70f082239d5",
   "metadata": {},
   "source": [
    "Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab73104a-ab35-4f0a-adbf-266d12c57f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"coffee_roast_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85da0681-b77b-4020-b8f0-3b65f724cc09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

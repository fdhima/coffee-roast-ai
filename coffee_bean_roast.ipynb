{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36d942e8-eb14-4bc8-ac05-417ed0503a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-05 17:19:13.527792: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25260213-d4d5-4a72-a4d9-bf058114d6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('coffee_beans.csv')\n",
    "X = df[\"filepaths\"]\n",
    "y = df[\"class index\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe517ecc-4761-43a5-bf0b-6dee0427da03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(x):\n",
    "    images = []\n",
    "    for path in x:\n",
    "        img = cv2.imread(path)\n",
    "        if img is None:\n",
    "            print(f\"Could not read image: {path}\")\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (224, 224))\n",
    "        img = img / 255.0  # normalize\n",
    "    \n",
    "        images.append(img)\n",
    "    \n",
    "    images = np.array(images, dtype=np.float32)\n",
    "    return images\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ef8d24-3483-40c5-bf9b-a4ec9b2cc68c",
   "metadata": {},
   "source": [
    "Split dataset into training, validation and test set. This split will be used to choose the best neural network architecture for the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "416ff638-5ed4-4f36-96f4-87cb692b2b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, x_, y_train, y_ = train_test_split(X , y, test_size=0.40, random_state=1)\n",
    "X_cv, X_test, y_cv, y_test = train_test_split(x_ , y_, test_size=0.20, random_state=1)\n",
    "\n",
    "del x_, y_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ce0c5c-4a10-48b7-8a92-54b6dadd6441",
   "metadata": {},
   "source": [
    "Load the images defined in the $x$ sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c4c62ef-2edb-4c26-b42e-72efcda27fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = load_images(X_train)\n",
    "X_cv = load_images(X_cv)\n",
    "X_test = load_images(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "733e7b24-4efe-44fb-bfdb-2b98303cbacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2   \n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "tf.keras.utils.set_random_seed(42) # seed for reproducibility\n",
    "\n",
    "# Pre Trained CNN\n",
    "base_model = MobileNetV2(\n",
    "    input_shape=(224, 224, 3),\n",
    "    include_top=False,\n",
    "    weights='imagenet'\n",
    ")\n",
    "\n",
    "base_model.trainable = False  # freeze pretrained weights\n",
    "\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.BatchNormalization(),\n",
    "\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "\n",
    "    layers.Dense(4, activation='linear')\n",
    "])\n",
    "\n",
    "nn_models = [model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0252840-c8bc-4e11-a059-dcb7466c3443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training sequential_1...\n",
      "Epoch 1/10\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 674ms/step - accuracy: 0.8823 - loss: 1.1651\n",
      "Epoch 2/10\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 635ms/step - accuracy: 0.9563 - loss: 0.7511\n",
      "Epoch 3/10\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 629ms/step - accuracy: 0.9615 - loss: 0.4477\n",
      "Epoch 4/10\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 704ms/step - accuracy: 0.9792 - loss: 0.5493\n",
      "Epoch 5/10\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 668ms/step - accuracy: 0.9875 - loss: 0.2512\n",
      "Epoch 6/10\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 650ms/step - accuracy: 0.9781 - loss: 0.3299\n",
      "Epoch 7/10\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 652ms/step - accuracy: 0.9854 - loss: 0.3147\n",
      "Epoch 8/10\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 648ms/step - accuracy: 0.9781 - loss: 0.3852\n",
      "Epoch 9/10\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 677ms/step - accuracy: 0.9760 - loss: 0.6257\n",
      "Epoch 10/10\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 654ms/step - accuracy: 0.9854 - loss: 0.5006\n",
      "Done\n",
      "\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 650ms/step\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 669ms/step\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "nn_train_cross_entropy = []\n",
    "nn_cv_cross_entropy = []\n",
    "\n",
    "for model in nn_models:\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), # gradient descent optimatiation\n",
    "        loss= tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=['accuracy'], # ??? have to research\n",
    "    )\n",
    "    print(f\"Training {model.name}...\")\n",
    "    \n",
    "    model.fit(X_train, y_train, epochs=10)\n",
    "    print(\"Done\\n\")\n",
    "\n",
    "    # Instantiate loss function\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    \n",
    "    # Record the training Log Losses\n",
    "    yhat = model.predict(X_train)\n",
    "    # pred_class = np.argmax(yhat, axis=1)[0]\n",
    "    train_cross_entropy = loss_fn(y_train, yhat).numpy()\n",
    "    nn_train_cross_entropy.append(train_cross_entropy)\n",
    "\n",
    "    # Record the cross validation Log Losses\n",
    "    yhat = model.predict(X_cv)\n",
    "    # pred_class = np.argmax(yhat, axis=1)[0]\n",
    "    cv_cross_entropy = loss_fn(y_cv, yhat).numpy()\n",
    "    nn_cv_cross_entropy.append(cv_cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed454b9-f3e8-4523-8108-efe7fc605b08",
   "metadata": {},
   "source": [
    "Because softmax is integrated into the output layer the output s a vector of probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e893e94-805c-4a69-b109-18d73ced1aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequential_1 Performance Summary\n",
      "-----------------------------\n",
      "Training Cross-Entropy Loss : 0.0252\n",
      "CV Cross-Entropy Loss       : 1.1881\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model in nn_models:\n",
    "    print(\n",
    "        f\"{model.name} Performance Summary\\n\"\n",
    "        f\"-----------------------------\\n\"\n",
    "        f\"Training Cross-Entropy Loss : {nn_train_cross_entropy[-1]:.4f}\\n\"\n",
    "        f\"CV Cross-Entropy Loss       : {nn_cv_cross_entropy[-1]:.4f}\\n\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4320e64f-b4c7-46cb-ac66-d0d6355f0ef6",
   "metadata": {},
   "source": [
    "Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4c4742e-b7b2-4400-8895-83ff9fd7d353",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = {\n",
    "    0: \"Dark\",\n",
    "    1: \"Green\",\n",
    "    2: \"Light\",\n",
    "    3: \"Medium\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f174df5-ffa7-4917-a809-e3ea2af9a919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Predicted: Dark\n",
      "Class probabilities (Dark, Green, Light, Medium): [ 176.53752  -165.09375  -241.84581   -25.023415]\n"
     ]
    }
   ],
   "source": [
    "IMG_SIZE = 224\n",
    "\n",
    "img = cv2.imread(\"test/Dark/dark (14).png\")\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "img = img / 255.0\n",
    "\n",
    "img = np.expand_dims(img, axis=0)  # shape (1, 224, 224, 3)\n",
    "\n",
    "pred = model.predict(img)\n",
    "pred_class = np.argmax(pred, axis=1)[0]\n",
    "\n",
    "print(\"Predicted:\", class_names[pred_class])\n",
    "print(\"Class probabilities (Dark, Green, Light, Medium):\", pred[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ed5755-261c-405e-8deb-c70f082239d5",
   "metadata": {},
   "source": [
    "Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab73104a-ab35-4f0a-adbf-266d12c57f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"coffee_roast_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85da0681-b77b-4020-b8f0-3b65f724cc09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
